
openAI key:

name - my demo key
key - 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

------------------------------------------------------------------------------------
                        FLOW OF THE PROJECT (without openai library)
------------------------------------------------------------------------------------

- create virtual environment (using pipenv) ----- pipenv --python 3.9
- activate virtual environment ----- pipenv shell
- install fastapi with helper packages (jinja2, uvicorn, python-dotenv) ----- pip install "fastapi[all]"
- install request library to send request to OpenAI endpoints ----- pip install requests


------------------------------------------------------------------------------------
                        FLOW OF THE PROJECT (with openai library)
------------------------------------------------------------------------------------

- install openai library ----- pip install openai

------------------------------------------------------------------------------------
                    You exceeded your current quota PROBLEM
------------------------------------------------------------------------------------

- as it seems if your account was created a while ago, your free tokens will be expried 
- you can create new account for free tokens but consider free tokens are given based on mobile phone (not on mail)


------------------------------------------------------------------------------------
                        TEXT COMPLETION OR CHAT COMPLETION
------------------------------------------------------------------------------------

- TEXT - for one time task. if you want to have history or questions / tasks relatied to each other the we need CHAT COMPLETION

// https://platform.openai.com/docs/guides/completion/prompt-design
- API is non-deterministic by default - Setting temperature to 0 will make the outputs mostly deterministic
- three basic guidelines for prompt design: Show and tell, Provide quality data, Check your settings (The temperature and top_p settings control how deterministic the
  model is in generating a response. The number one mistake people use with these settings is assuming that they're "cleverness" or "creativity" controls)
/// Clasification ///  
- paying attenton several to seveal features:
    - Use plain language to describe your inputs and outputs (because we are middle man between user and model, we can turn 
                                                              their text to more plain text)
    - Show the API how to respond to any case (for example, answer should be positive, neutral, or negative)
    - You need fewer examples for familiar tasks (is concept you are talking about already known API or not)
- you can use playground to find best temperature and top_p for your concept and theme.
/// Improving Efficienct ///
- Make sure your probability setting is calibrated correctly by running multiple tests.
- Don't make your list too long or the API is likely to drift.
/// Conversation ///
- We tell the API the intent but we also tell it how to behave (we give them example and tell: The assistant is helpful, 
                                                                creative, clever, and very friendly.)
- We give the API an identity (The following is a conversation with an AI assistant)
/// Factual Responses ///
- Provide a ground truth for the API
- Use a low probability and show the API how to say "I don't know"


/// CHAT COMPLETION ///
/// https://platform.openai.com/docs/guides/chat ///

- Chat models take a series of messages as input, and return a model-generated message as output
- Although the chat format is designed to make multi-turn conversations easy
- each object has a role (either "system", "user", or "assistant")
- Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages. 
  (The system message helps set the behavior of the assistant) (gpt-3.5-turbo-0301 does not always pay strong attention to system messages. Future models will be trained to pay stronger attention to system messages.)
-  The user messages help instruct the assistant. They can be generated by the end users of an application, or set by a developer as an instruction.
- The assistant messages help store prior responses. They can also be written by a developer to help give examples of desired behavior.
/// RESPONSE FORMAT ///
- Every response will include a finish_reason. The possible values for finish_reason are:
    - stop: API returned complete model output
    - length: Incomplete model output due to max_tokens parameter or token limit
    - content_filter: Omitted content due to a flag from our content filters
    - null: API response still in progress or incomplete
/// MANAGING TOKENS ///
- In English, a token can be as short as one character or as long as one word
- The total number of tokens in an API call affects:
    - How much your API call costs, as you pay per token
    - How long your API call takes
- Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens 
  in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. 
- get number of tokens, tiktoken python library (https://github.com/openai/tiktoken)
- If a conversation has too many tokens to fit within a modelâ€™s maximum limit (e.g., more than 4096 tokens for gpt-3.5-turbo), 
  you will have to truncate, omit, or otherwise shrink your text until it fits.
- Note too that very long conversations are more likely to receive incomplete replies (if api call tokens is 4090, answer can be just 6 tokens)
- 


- Because gpt-3.5-turbo performs at a similar capability to text-davinci-003 but at 10% the 
  price per token, we recommend gpt-3.5-turbo for most use cases.


------------------------------------------------------------------------------------
                        TECHNIQUES TO IMPROVE RELIABILITY
------------------------------------------------------------------------------------

- https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md



------------------------------------------------------------------------------------
                        TOPICS DURING DEVELOPMENT
------------------------------------------------------------------------------------

/// how to ask in what format should the response be returned?  ///
- first way is to describe response format using "role": "system"
- second way is to show examples
- let's see both examples, second one is much effective. 